{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"execution":{"iopub.status.busy":"2023-03-31T17:33:34.330915Z","iopub.execute_input":"2023-03-31T17:33:34.331401Z","iopub.status.idle":"2023-03-31T17:33:34.336043Z","shell.execute_reply.started":"2023-03-31T17:33:34.331326Z","shell.execute_reply":"2023-03-31T17:33:34.335030Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sys, getopt\nimport os\nfrom glob import glob\nfrom keras.preprocessing.image import ImageDataGenerator \nfrom tensorflow.keras.utils import array_to_img\nfrom tensorflow.keras.utils import img_to_array\nfrom tensorflow.keras.utils import load_img\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import *\nfrom keras.layers import *\nimport keras.backend as K\nfrom keras import optimizers\nfrom keras.activations import *\nimport tensorflow as tf\nimport gc\nimport itertools\nimport cv2\nimport math\nimport matplotlib.pyplot as plt\nfrom sys import getsizeof\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:30:52.643768Z","iopub.execute_input":"2023-04-03T18:30:52.644279Z","iopub.status.idle":"2023-04-03T18:31:00.684510Z","shell.execute_reply.started":"2023-04-03T18:30:52.644231Z","shell.execute_reply":"2023-04-03T18:31:00.683264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:00.686543Z","iopub.execute_input":"2023-04-03T18:31:00.687368Z","iopub.status.idle":"2023-04-03T18:31:00.878524Z","shell.execute_reply.started":"2023-04-03T18:31:00.687325Z","shell.execute_reply":"2023-04-03T18:31:00.877299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Reading in Data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8b7295f0-4fd5-4fd6-b0d0-dd3d9bfd4303","_cell_guid":"038f2c39-c48a-4cef-bb4d-9b9aa4b82822","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-03T18:31:00.880103Z","iopub.execute_input":"2023-04-03T18:31:00.880796Z","iopub.status.idle":"2023-04-03T18:31:00.907283Z","shell.execute_reply.started":"2023-04-03T18:31:00.880747Z","shell.execute_reply":"2023-04-03T18:31:00.906177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_weights = f\"/kaggle/input/tracknetpickleball/model906_30\"\nsave_weights = \"/kaggle/working/dense_weights\"\ndataDir = f\"/kaggle/input/tracknetpickleball/y_data_30\"","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:00.910004Z","iopub.execute_input":"2023-04-03T18:31:00.910451Z","iopub.status.idle":"2023-04-03T18:31:00.916468Z","shell.execute_reply.started":"2023-04-03T18:31:00.910413Z","shell.execute_reply":"2023-04-03T18:31:00.915242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = os.path.abspath(os.path.join(dataDir))\npath = glob(os.path.join(r, '*.npy'))\nnum = len(path) / 2\nidx = np.arange(num, dtype='int') + 1","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:00.918167Z","iopub.execute_input":"2023-04-03T18:31:00.918566Z","iopub.status.idle":"2023-04-03T18:31:00.926872Z","shell.execute_reply.started":"2023-04-03T18:31:00.918499Z","shell.execute_reply":"2023-04-03T18:31:00.925755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Model Training Hyperparameters","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE=10\nHEIGHT=288\nWIDTH=512\nepochs = 20\ntol = 15\n\n# keep very small for unfreezing\nlr = 0.01","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:00.928569Z","iopub.execute_input":"2023-04-03T18:31:00.929086Z","iopub.status.idle":"2023-04-03T18:31:00.936074Z","shell.execute_reply.started":"2023-04-03T18:31:00.929049Z","shell.execute_reply":"2023-04-03T18:31:00.935027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Utility Functions","metadata":{}},{"cell_type":"code","source":"#Loss function\ndef custom_loss(y_true, y_pred):\n\tloss = (-1)*(K.square(1 - y_pred) * y_true * K.log(K.clip(y_pred, K.epsilon(), 1)) + K.square(y_pred) * (1 - y_true) * K.log(K.clip(1 - y_pred, K.epsilon(), 1)))\n\treturn K.mean(loss)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:00.937633Z","iopub.execute_input":"2023-04-03T18:31:00.938211Z","iopub.status.idle":"2023-04-03T18:31:00.945863Z","shell.execute_reply.started":"2023-04-03T18:31:00.938170Z","shell.execute_reply":"2023-04-03T18:31:00.944743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def TrackNet3(input_height, input_width): #input_height = 288, input_width = 512\n\timgs_input = Input(shape=(9,input_height,input_width))\n\t#Layer1\n\tx = Conv2D(64, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first' )(imgs_input)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#Layer2\n\tx = Conv2D(64, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first' )(x)\n\tx = ( Activation('relu'))(x)\n\tx1 = ( BatchNormalization())(x)\n\t#Layer3\n\tx = MaxPooling2D((2, 2), strides=(2, 2), data_format='channels_first' )(x1)\n\t#Layer4\n\tx = Conv2D(128, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first' )(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#Layer5\n\tx = Conv2D(128, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first' )(x)\n\tx = ( Activation('relu'))(x)\n\tx2 = ( BatchNormalization())(x)\n\t#x2 = (Dropout(0.5))(x2) \n\t#Layer6\n\tx = MaxPooling2D((2, 2), strides=(2, 2), data_format='channels_first' )(x2)\n\t#Layer7\n\tx = Conv2D(256, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first' )(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#Layer8\n\tx = Conv2D(256, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first' )(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#Layer9\n\tx = Conv2D(256, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first' )(x)\n\tx = ( Activation('relu'))(x)\n\tx3 = ( BatchNormalization())(x)\n\t#x3 = (Dropout(0.5))(x3)\n\t#Layer10\n\tx = MaxPooling2D((2, 2), strides=(2, 2), data_format='channels_first' )(x3)\n\t#Layer11\n\tx = ( Conv2D(512, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first'))(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#Layer12\n\tx = ( Conv2D(512, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first'))(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#Layer13\n\tx = ( Conv2D(512, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first'))(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#x = (Dropout(0.5))(x)\n\t#Layer14\n\t#x = UpSampling2D( (2,2), data_format='channels_first')(x)\n\tx = concatenate( [UpSampling2D( (2,2), data_format='channels_first')(x), x3], axis=1)\n\t#Layer15\n\tx = ( Conv2D( 256, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first'))(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#Layer16\n\tx = ( Conv2D( 256, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first'))(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#Layer17\n\tx = ( Conv2D( 256, (3, 3), kernel_initializer='random_uniform', padding='same', data_format='channels_first'))(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t\n\t#Layer18\n\t#x = UpSampling2D( (2,2), data_format='channels_first')(x)\n\tx = concatenate( [UpSampling2D( (2,2), data_format='channels_first')(x), x2], axis=1)\n\t#Layer19\n\tx = ( Conv2D( 128 , (3, 3), kernel_initializer='random_uniform', padding='same' , data_format='channels_first' ))(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#Layer20\n\tx = ( Conv2D( 128 , (3, 3), kernel_initializer='random_uniform', padding='same' , data_format='channels_first' ))(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#Layer21\n\t#x = UpSampling2D( (2,2), data_format='channels_first')(x)\n\tx = concatenate( [UpSampling2D( (2,2), data_format='channels_first')(x), x1], axis=1)\n\t#Layer22\n\tx = ( Conv2D( 64 , (3, 3), kernel_initializer='random_uniform', padding='same'  , data_format='channels_first' ))(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#Layer23\n\tx = ( Conv2D( 64 , (3, 3), kernel_initializer='random_uniform', padding='same'  , data_format='channels_first' ))(x)\n\tx = ( Activation('relu'))(x)\n\tx = ( BatchNormalization())(x)\n\t#Layer24\n\tx =  Conv2D( 3 , (1, 1) , kernel_initializer='random_uniform', padding='same', data_format='channels_first' )(x)\n\tx = ( Activation('sigmoid'))(x)\n        \n\to_shape = Model(imgs_input , x ).output_shape\n\t#print (\"layer24 output shape:\", o_shape[1],o_shape[2],o_shape[3])\n\t#Layer24 output shape: (3, 288, 512)\n\tOutputHeight = o_shape[2]\n\tOutputWidth = o_shape[3]\n\toutput = x\n\tmodel = Model( imgs_input , output)\n\t#model input unit:9*288*512, output unit:3*288*512\n\tmodel.outputWidth = OutputWidth\n\tmodel.outputHeight = OutputHeight\n\t#Show model's details\n\t#model.summary()\n\treturn model","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-04-03T18:31:00.947417Z","iopub.execute_input":"2023-04-03T18:31:00.948106Z","iopub.status.idle":"2023-04-03T18:31:00.972740Z","shell.execute_reply.started":"2023-04-03T18:31:00.948064Z","shell.execute_reply":"2023-04-03T18:31:00.971755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Return the numbers of true positive, true negative, false positive and false negative\ndef outcome(y_pred, y_true, tol):\n\tn = y_pred.shape[0]\n\ti = 0\n\tTP = TN = FP1 = FP2 = FN = 0\n\twhile i < n:\n\t\tfor j in range(3):\n\t\t\tif np.amax(y_pred[i][j]) == 0 and np.amax(y_true[i][j]) == 0:\n\t\t\t\tTN += 1\n\t\t\telif np.amax(y_pred[i][j]) > 0 and np.amax(y_true[i][j]) == 0:\n\t\t\t\tFP2 += 1\n\t\t\telif np.amax(y_pred[i][j]) == 0 and np.amax(y_true[i][j]) > 0:\n\t\t\t\tFN += 1\n\t\t\telif np.amax(y_pred[i][j]) > 0 and np.amax(y_true[i][j]) > 0:\n\t\t\t\th_pred = y_pred[i][j] * 255\n\t\t\t\th_true = y_true[i][j] * 255\n\t\t\t\th_pred = h_pred.astype('uint8')\n\t\t\t\th_true = h_true.astype('uint8')\n\t\t\t\t#h_pred\n\t\t\t\t(cnts, _) = cv2.findContours(h_pred.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\t\t\t\trects = [cv2.boundingRect(ctr) for ctr in cnts]\n\t\t\t\tmax_area_idx = 0\n\t\t\t\tmax_area = rects[max_area_idx][2] * rects[max_area_idx][3]\n\t\t\t\tfor j in range(len(rects)):\n\t\t\t\t\tarea = rects[j][2] * rects[j][3]\n\t\t\t\t\tif area > max_area:\n\t\t\t\t\t\tmax_area_idx = j\n\t\t\t\t\t\tmax_area = area\n\t\t\t\ttarget = rects[max_area_idx]\n\t\t\t\t(cx_pred, cy_pred) = (int(target[0] + target[2] / 2), int(target[1] + target[3] / 2))\n\t\t\t\t#h_true\n\t\t\t\t(cnts, _) = cv2.findContours(h_true.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\t\t\t\trects = [cv2.boundingRect(ctr) for ctr in cnts]\n\t\t\t\tmax_area_idx = 0\n\t\t\t\tmax_area = rects[max_area_idx][2] * rects[max_area_idx][3]\n\t\t\t\tfor j in range(len(rects)):\n\t\t\t\t\tarea = rects[j][2] * rects[j][3]\n\t\t\t\t\tif area > max_area:\n\t\t\t\t\t\tmax_area_idx = j\n\t\t\t\t\t\tmax_area = area\n\t\t\t\ttarget = rects[max_area_idx]\n\t\t\t\t(cx_true, cy_true) = (int(target[0] + target[2] / 2), int(target[1] + target[3] / 2))\n\t\t\t\tdist = math.sqrt(pow(cx_pred-cx_true, 2)+pow(cy_pred-cy_true, 2))\n\t\t\t\tif dist > tol:\n\t\t\t\t\tFP1 += 1\n\t\t\t\telse:\n\t\t\t\t\tTP += 1\n\t\ti += 1\n\treturn (TP, TN, FP1, FP2, FN)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:00.976466Z","iopub.execute_input":"2023-04-03T18:31:00.976782Z","iopub.status.idle":"2023-04-03T18:31:00.992532Z","shell.execute_reply.started":"2023-04-03T18:31:00.976752Z","shell.execute_reply":"2023-04-03T18:31:00.991395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Return the values of accuracy, precision and recall\ndef evaluation(outcome):\n    (TP, TN, FP1, FP2, FN) = outcome\n    # does this implictly unpack?\n    try:\n        accuracy = (TP + TN) / (TP + TN + FP1 + FP2 + FN)\n    except:\n        accuracy = 0\n    try:\n        precision = TP / (TP + FP1 + FP2)\n    except:\n        precision = 0\n    try:\n        recall = TP / (TP + FN)\n    except:\n        recall = 0\n    return (accuracy, precision, recall)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:00.998749Z","iopub.execute_input":"2023-04-03T18:31:00.999051Z","iopub.status.idle":"2023-04-03T18:31:01.006644Z","shell.execute_reply.started":"2023-04-03T18:31:00.999009Z","shell.execute_reply":"2023-04-03T18:31:01.004959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Load Base TrackNetV2","metadata":{}},{"cell_type":"code","source":"# load old model\n# all params are currently unfrozen\nbase_model = load_model(load_weights, custom_objects={'custom_loss':custom_loss})\n\nADADELTA = optimizers.Adadelta(learning_rate=lr) # 1e-5 if fine tuning\nbase_model.compile(loss=custom_loss,optimizer=ADADELTA)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:01.008782Z","iopub.execute_input":"2023-04-03T18:31:01.009149Z","iopub.status.idle":"2023-04-03T18:31:06.386310Z","shell.execute_reply.started":"2023-04-03T18:31:01.009106Z","shell.execute_reply":"2023-04-03T18:31:06.385227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(base_model.layers)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.387684Z","iopub.execute_input":"2023-04-03T18:31:06.388076Z","iopub.status.idle":"2023-04-03T18:31:06.396472Z","shell.execute_reply.started":"2023-04-03T18:31:06.388038Z","shell.execute_reply":"2023-04-03T18:31:06.395047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainable_params = np.sum([K.count_params(w) for w in base_model.trainable_weights])\ntrainable_params","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.398193Z","iopub.execute_input":"2023-04-03T18:31:06.398861Z","iopub.status.idle":"2023-04-03T18:31:06.409430Z","shell.execute_reply.started":"2023-04-03T18:31:06.398819Z","shell.execute_reply":"2023-04-03T18:31:06.408065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_trainable_params = np.sum([K.count_params(w) for w in base_model.non_trainable_weights])\nnon_trainable_params","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.411449Z","iopub.execute_input":"2023-04-03T18:31:06.411992Z","iopub.status.idle":"2023-04-03T18:31:06.422684Z","shell.execute_reply.started":"2023-04-03T18:31:06.411952Z","shell.execute_reply":"2023-04-03T18:31:06.421631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Train/Val split (70/30)","metadata":{}},{"cell_type":"code","source":"npy_batch_size = 250","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.424204Z","iopub.execute_input":"2023-04-03T18:31:06.424930Z","iopub.status.idle":"2023-04-03T18:31:06.430140Z","shell.execute_reply.started":"2023-04-03T18:31:06.424888Z","shell.execute_reply":"2023-04-03T18:31:06.428926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_idx_start = math.floor(.70 * npy_batch_size)\nval_idx_start","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.431881Z","iopub.execute_input":"2023-04-03T18:31:06.432825Z","iopub.status.idle":"2023-04-03T18:31:06.441090Z","shell.execute_reply.started":"2023-04-03T18:31:06.432786Z","shell.execute_reply":"2023-04-03T18:31:06.439900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = np.arange(0,val_idx_start)\nval = np.arange(val_idx_start, npy_batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.442834Z","iopub.execute_input":"2023-04-03T18:31:06.443506Z","iopub.status.idle":"2023-04-03T18:31:06.448547Z","shell.execute_reply.started":"2023-04-03T18:31:06.443467Z","shell.execute_reply":"2023-04-03T18:31:06.447431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Base Model Performance","metadata":{}},{"cell_type":"code","source":"base_metrics = pd.DataFrame(index = [\"train\", \"val\", \"all\"], columns=[\"TP\",\"TN\", \"FP1\", \"FP2\", \"FN\", \"loss\", \"acc\", \"prec\", \"rec\"], dtype=\"float64\")\nbase_metrics = base_metrics.fillna(0)\nbase_metrics   ","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.450318Z","iopub.execute_input":"2023-04-03T18:31:06.450997Z","iopub.status.idle":"2023-04-03T18:31:06.477866Z","shell.execute_reply.started":"2023-04-03T18:31:06.450956Z","shell.execute_reply":"2023-04-03T18:31:06.477004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"EVALUATING BASE MODEL PERFORMANCE\")\n# for file_num in idx:\n#     print(f\"\\tReading in file: x_data_{str(file_num)}.npy\")\n#     X = np.load(os.path.abspath(os.path.join(dataDir, 'x_data_' + str(file_num) + '.npy')))\n\n#     print(\"\\tMaking predictions\")\n#     y_hat = base_model.predict(X, batch_size=BATCH_SIZE)\n\n#     print(f\"\\tDeleting x_data_{str(file_num)}.npy\")\n#     del X\n#     gc.collect()\n\n#     print(f\"\\tReading in file: y_data_{str(file_num)}.npy\")\n#     y = np.load(os.path.abspath(os.path.join(dataDir, 'y_data_' + str(file_num) + '.npy')))\n\n#     print(\"\\tCalculating train/val/overall loss\")\n#     base_metrics.loc[\"train\", \"loss\"] += custom_loss(y[train], y_hat[train]).numpy()\n#     base_metrics.loc[\"val\", \"loss\"] += custom_loss(y[val], y_hat[val]).numpy()\n#     base_metrics.loc[\"all\", \"loss\"] += custom_loss(y, y_hat).numpy()\n\n#     print(\"\\tCalculating train/val/overall classification performance\")\n#     y_pred = (y_hat > 0.5).astype(\"float32\")\n#     base_metrics.loc[\"train\",[\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]] += outcome(y_pred[train], y[train], tol)\n#     base_metrics.loc[\"val\", [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]] += outcome(y_pred[val], y[val], tol)\n#     base_metrics.loc[\"all\", [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]] += outcome(y_pred, y, tol)\n\n#     print(f\"\\tDeleting y_data_{str(file_num)}.npy\")\n#     del y\n#     del y_hat\n#     del y_pred\n#     gc.collect()\n\n# print(\"\\tCalculating aggregate metrics\")\n# for subset in [\"train\", \"val\", \"all\"]:\n#     # acc, prec, rec\n#     base_metrics.loc[subset, [\"acc\", \"prec\", \"rec\"]] = evaluation(base_metrics.loc[subset,[\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]])\n\n#     # standardize TP, FP, ...\n#     base_metrics.loc[subset, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]] /= base_metrics.loc[subset, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]].sum()\n\n#     # make into percentages\n#     base_metrics.loc[subset, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\", \"acc\", \"prec\", \"rec\"]] = (base_metrics.loc[subset, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\", \"acc\", \"prec\", \"rec\"]]*100).round(2)\n\n# print(\"BASE METRICS\")\n# print(base_metrics)  \n# print()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.480885Z","iopub.execute_input":"2023-04-03T18:31:06.481159Z","iopub.status.idle":"2023-04-03T18:31:06.487153Z","shell.execute_reply.started":"2023-04-03T18:31:06.481133Z","shell.execute_reply":"2023-04-03T18:31:06.485877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BASE METRICS (files 1-30, 250 examples each)\n#           TP     TN   FP1   FP2     FN      loss    acc   prec    rec\n# train  51.68  10.37  5.33  2.32  30.30  0.009474  62.05  87.11  63.04\n# val    50.47  13.79  4.46  2.55  28.73  0.009288  64.27  87.81  63.73\n# all    51.32  11.40  5.07  2.39  29.83  0.009418  62.72  87.31  63.24","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.489246Z","iopub.execute_input":"2023-04-03T18:31:06.489556Z","iopub.status.idle":"2023-04-03T18:31:06.497697Z","shell.execute_reply.started":"2023-04-03T18:31:06.489520Z","shell.execute_reply":"2023-04-03T18:31:06.496509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Option 2: Freeze everything except last k conv layers","metadata":{}},{"cell_type":"code","source":"k = 14","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.499983Z","iopub.execute_input":"2023-04-03T18:31:06.500492Z","iopub.status.idle":"2023-04-03T18:31:06.506489Z","shell.execute_reply.started":"2023-04-03T18:31:06.500451Z","shell.execute_reply":"2023-04-03T18:31:06.505308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv_indices = [idx for idx, layer in enumerate(base_model.layers) if \"conv\" in layer.name]\nconv_indices","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.508068Z","iopub.execute_input":"2023-04-03T18:31:06.508678Z","iopub.status.idle":"2023-04-03T18:31:06.517936Z","shell.execute_reply.started":"2023-04-03T18:31:06.508636Z","shell.execute_reply":"2023-04-03T18:31:06.516659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of conv layers that can be fine tuned\nlen(conv_indices)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.519545Z","iopub.execute_input":"2023-04-03T18:31:06.520032Z","iopub.status.idle":"2023-04-03T18:31:06.530237Z","shell.execute_reply.started":"2023-04-03T18:31:06.519986Z","shell.execute_reply":"2023-04-03T18:31:06.529010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freeze_to = conv_indices[-k]\nfreeze_to","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.531841Z","iopub.execute_input":"2023-04-03T18:31:06.532305Z","iopub.status.idle":"2023-04-03T18:31:06.540214Z","shell.execute_reply.started":"2023-04-03T18:31:06.532268Z","shell.execute_reply":"2023-04-03T18:31:06.538938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in base_model.layers[0:freeze_to]:\n    layer.trainable = False\nfor layer in base_model.layers[freeze_to:]:\n    layer.trainable= True","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.541956Z","iopub.execute_input":"2023-04-03T18:31:06.542420Z","iopub.status.idle":"2023-04-03T18:31:06.551604Z","shell.execute_reply.started":"2023-04-03T18:31:06.542376Z","shell.execute_reply":"2023-04-03T18:31:06.550472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for l in base_model.layers:\n    print(f\"{l.trainable}: {l.name}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.553345Z","iopub.execute_input":"2023-04-03T18:31:06.553931Z","iopub.status.idle":"2023-04-03T18:31:06.562103Z","shell.execute_reply.started":"2023-04-03T18:31:06.553841Z","shell.execute_reply":"2023-04-03T18:31:06.560784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compile the new model\nmodel = base_model\nADADELTA = optimizers.Adadelta(learning_rate=lr) # 1e-5 if fine tuning\nmodel.compile(loss=custom_loss, optimizer=ADADELTA)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.564198Z","iopub.execute_input":"2023-04-03T18:31:06.565121Z","iopub.status.idle":"2023-04-03T18:31:06.577954Z","shell.execute_reply.started":"2023-04-03T18:31:06.565083Z","shell.execute_reply":"2023-04-03T18:31:06.577133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainable_params = np.sum([K.count_params(w) for w in model.trainable_weights])\ntrainable_params","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.584372Z","iopub.execute_input":"2023-04-03T18:31:06.584642Z","iopub.status.idle":"2023-04-03T18:31:06.592732Z","shell.execute_reply.started":"2023-04-03T18:31:06.584615Z","shell.execute_reply":"2023-04-03T18:31:06.591465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_trainable_params = np.sum([K.count_params(w) for w in model.non_trainable_weights])\nnon_trainable_params","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.594577Z","iopub.execute_input":"2023-04-03T18:31:06.595411Z","iopub.status.idle":"2023-04-03T18:31:06.605821Z","shell.execute_reply.started":"2023-04-03T18:31:06.595372Z","shell.execute_reply":"2023-04-03T18:31:06.604782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Fine-Tune the Model","metadata":{}},{"cell_type":"code","source":"train_metrics = pd.DataFrame(columns = [\"TP\",\"TN\", \"FP1\", \"FP2\", \"FN\", \"loss\", \"acc\", \"prec\", \"rec\"], index = np.arange(1, epochs+1))\ntrain_metrics = train_metrics.fillna(0)\ntrain_metrics","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.606849Z","iopub.execute_input":"2023-04-03T18:31:06.607109Z","iopub.status.idle":"2023-04-03T18:31:06.626558Z","shell.execute_reply.started":"2023-04-03T18:31:06.607084Z","shell.execute_reply":"2023-04-03T18:31:06.625730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_metrics = pd.DataFrame(columns = [\"TP\",\"TN\", \"FP1\", \"FP2\", \"FN\", \"loss\", \"acc\", \"prec\", \"rec\"], index = np.arange(1, epochs+1))\nval_metrics = val_metrics.fillna(0)\nval_metrics","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.628630Z","iopub.execute_input":"2023-04-03T18:31:06.629358Z","iopub.status.idle":"2023-04-03T18:31:06.646135Z","shell.execute_reply.started":"2023-04-03T18:31:06.629319Z","shell.execute_reply":"2023-04-03T18:31:06.645001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Beginning training......')\nfor epoch in range(1, epochs+1):\n    print('============ epoch', epoch, '================')\n    \n    print(\"TRAINING\")\n    for file_num in idx:\n        print(f\"\\tReading in files: x_data_{str(file_num)}.npy, y_data_{str(file_num)}.npy\")\n        # read in big batch (500 examples)\n        X = np.load(os.path.abspath(os.path.join(dataDir, 'x_data_' + str(file_num) + '.npy')))\n        y = np.load(os.path.abspath(os.path.join(dataDir, 'y_data_' + str(file_num) + '.npy')))\n        \n        print(\"\\tFitting model with 70% train\")\n        # fit in smaller batches (3 examples)\n        history = model.fit(X[train], \n                            y[train], \n                            batch_size=BATCH_SIZE, \n                            epochs=1,\n                            shuffle=False)\n        \n        print(f\"\\tDeleting x_data_{str(file_num)}.npy and y_data_{str(file_num)}.npy\")\n        del X\n        del y\n        gc.collect()\n        \n    print(\"EVALUATING\")\n    for file_num in idx:\n        print(f\"\\tReading in file: x_data_{str(file_num)}.npy\")\n        X = np.load(os.path.abspath(os.path.join(dataDir, 'x_data_' + str(file_num) + '.npy')))\n\n        print(\"\\tMaking predictions\")\n        y_hat = model.predict(X, batch_size=BATCH_SIZE)\n        \n        print(f\"\\tDeleting x_data_{str(file_num)}.npy\")\n        del X\n        gc.collect()\n        \n        print(f\"\\tReading in file: y_data_{str(file_num)}.npy\")\n        y = np.load(os.path.abspath(os.path.join(dataDir, 'y_data_' + str(file_num) + '.npy')))\n\n        print(\"\\tCalculating train and valid loss\")\n        train_metrics.loc[epoch, \"loss\"] += custom_loss(y[train], y_hat[train]).numpy()\n        val_metrics.loc[epoch, \"loss\"] += custom_loss(y[val], y_hat[val]).numpy()\n        \n        print(\"\\tCalculating train and valid classification performance\")\n        y_pred = (y_hat > 0.5).astype(\"float32\")\n        train_metrics.loc[epoch, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]] += outcome(y_pred[train], y[train], tol)\n        val_metrics.loc[epoch, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]] += outcome(y_pred[val], y[val], tol)\n\n        print(f\"\\tDeleting y_data_{str(file_num)}.npy\")\n        del y\n        del y_hat\n        del y_pred\n        gc.collect()\n    \n    print(\"METRICS\")\n    # acc, prec, rec\n    train_metrics.loc[epoch, [\"acc\", \"prec\", \"rec\"]] = evaluation(train_metrics.loc[epoch,[\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]])\n    val_metrics.loc[epoch, [\"acc\", \"prec\", \"rec\"]] = evaluation(val_metrics.loc[epoch,[\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]])\n\n    # standardize TP, FP, ...\n    train_metrics.loc[epoch, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]] /= train_metrics.loc[epoch, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]].sum()\n    val_metrics.loc[epoch, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]] /= val_metrics.loc[epoch, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\"]].sum()\n\n    # make into percentages\n    train_metrics.loc[epoch, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\", \"acc\", \"prec\", \"rec\"]] = (train_metrics.loc[epoch, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\", \"acc\", \"prec\", \"rec\"]]*100).round(2)\n    val_metrics.loc[epoch, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\", \"acc\", \"prec\", \"rec\"]] = (val_metrics.loc[epoch, [\"TP\", \"TN\", \"FP1\", \"FP2\", \"FN\", \"acc\", \"prec\", \"rec\"]]*100).round(2)\n    \n    epoch_metrics = pd.concat([train_metrics.loc[epoch,:], val_metrics.loc[epoch,:]], axis=1)\n    epoch_metrics.columns = [\"train\", \"val\"]\n    print(epoch_metrics.astype('object')) \n    print()\n\n    # Save intermediate weights during training\n    model.save(save_weights + '_' + str(epoch))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T18:31:06.649187Z","iopub.execute_input":"2023-04-03T18:31:06.649458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Classification Performance Metrics","metadata":{}},{"cell_type":"code","source":"train_metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BASE METRICS (files 1-30, 250 examples each)\n#           TP     TN   FP1   FP2     FN      loss    acc   prec    rec\n# train  51.68  10.37  5.33  2.32  30.30  0.009474  62.05  87.11  63.04\n# val    50.47  13.79  4.46  2.55  28.73  0.009288  64.27  87.81  63.73\n# all    51.32  11.40  5.07  2.39  29.83  0.009418  62.72  87.31  63.24","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_perf = train_metrics.join(val_metrics, lsuffix=\"_train\", rsuffix=\"_val\")         \nclf_perf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_perf.to_csv(\"/kaggle/working/clf_perf.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare_train_val(metric, train_metrics, val_metrics):\n    plt.title(metric)\n    plt.xlabel('epoch')\n    plt.ylabel(\"%\")\n    x = np.arange(1,train_metrics.shape[0]+1,1)\n    plt.plot(x, train_metrics[metric], label=f\"train_{metric}\")\n    plt.plot(x, val_metrics[metric], label=f\"val_{metric}\")\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for metric in train_metrics.columns:\n    compare_train_val(metric, train_metrics, val_metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(save_weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}